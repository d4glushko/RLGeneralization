{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sys.path.append('sunblaze_envs')\n",
    "# import sunblaze_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 1\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        \n",
    "\n",
    "def train(env_name, score=199, training_episodes=10000000):\n",
    "#     env = sunblaze_envs.make(env_name)\n",
    "    env = gym.make(env_name)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn = Dqn(observation_space, action_space)\n",
    "    scores = []\n",
    "    all_scores = []\n",
    "    for i in range(1,training_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = dqn.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print(\"Run: \" + str(i) + \", exploration: \" + str(dqn.exploration_rate) + \", score: \" + str(step))\n",
    "                scores.append(step)\n",
    "                all_scores.append(step)\n",
    "                break\n",
    "            dqn.experience_replay()\n",
    "        if (i % 100 == 0):\n",
    "            avg = np.array(scores).mean()\n",
    "            print(\"===============\")\n",
    "            print(\"Episodes: \" + str(i) + \", mean 100 episodes reward: \" + str(avg))\n",
    "            print(\"===============\")\n",
    "            scores = []\n",
    "            if avg >= score:\n",
    "                break\n",
    "            \n",
    "    plt.plot(all_scores)\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n",
    "    return dqn\n",
    "\n",
    "def play(dqn, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        env.render()\n",
    "        action = dqn.act(state)\n",
    "        state_next, reward, terminal, info = env.step(action)\n",
    "        reward = reward if not terminal else -reward\n",
    "        state_next = np.reshape(state_next, [1, observation_space])\n",
    "        state = state_next\n",
    "        if terminal:\n",
    "            print(\"Score: \" + str(step))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Run: 1, exploration: 1.0, score: 32\n",
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/dmitriiglushko/UCU/ReinforcementLearning/Repos/ReinforcementLearning/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Run: 2, exploration: 0.918316468354365, score: 18\n",
      "Run: 3, exploration: 0.8560822709551227, score: 15\n",
      "Run: 4, exploration: 0.7514768435208588, score: 27\n",
      "Run: 5, exploration: 0.6730128848950395, score: 23\n",
      "Run: 6, exploration: 0.6337242817644086, score: 13\n",
      "Run: 7, exploration: 0.5704072587541458, score: 22\n",
      "Run: 8, exploration: 0.5082950737585841, score: 24\n",
      "Run: 9, exploration: 0.4858739637363176, score: 10\n",
      "Run: 10, exploration: 0.457510005540005, score: 13\n",
      "Run: 11, exploration: 0.43952667968844233, score: 9\n",
      "Run: 12, exploration: 0.42013897252428334, score: 10\n",
      "Run: 13, exploration: 0.3995984329713264, score: 11\n",
      "Run: 14, exploration: 0.38389143477919885, score: 9\n",
      "Run: 15, exploration: 0.3614809303671764, score: 13\n",
      "Run: 16, exploration: 0.2649210072611673, score: 63\n",
      "Run: 17, exploration: 0.22566020663225933, score: 33\n",
      "Run: 18, exploration: 0.19415447453059972, score: 31\n",
      "Run: 19, exploration: 0.15888051309497406, score: 41\n",
      "Run: 20, exploration: 0.1176130407830293, score: 61\n",
      "Run: 21, exploration: 0.0906265979439563, score: 53\n",
      "Run: 22, exploration: 0.08576489601717459, score: 12\n",
      "Run: 23, exploration: 0.06380744126877576, score: 60\n",
      "Run: 24, exploration: 0.04104898613852031, score: 89\n",
      "Run: 25, exploration: 0.021827832324362372, score: 127\n",
      "Run: 26, exploration: 0.012203600316175803, score: 117\n",
      "Run: 27, exploration: 0.01, score: 213\n",
      "Run: 28, exploration: 0.01, score: 239\n",
      "Run: 29, exploration: 0.01, score: 336\n",
      "Run: 30, exploration: 0.01, score: 229\n",
      "Run: 31, exploration: 0.01, score: 195\n",
      "Run: 32, exploration: 0.01, score: 176\n",
      "Run: 33, exploration: 0.01, score: 174\n",
      "Run: 34, exploration: 0.01, score: 132\n",
      "Run: 35, exploration: 0.01, score: 339\n",
      "Run: 36, exploration: 0.01, score: 140\n",
      "Run: 37, exploration: 0.01, score: 322\n",
      "Run: 38, exploration: 0.01, score: 164\n",
      "Run: 39, exploration: 0.01, score: 231\n",
      "Run: 40, exploration: 0.01, score: 379\n",
      "Run: 41, exploration: 0.01, score: 201\n",
      "Run: 42, exploration: 0.01, score: 151\n",
      "Run: 43, exploration: 0.01, score: 181\n",
      "Run: 44, exploration: 0.01, score: 326\n",
      "Run: 45, exploration: 0.01, score: 190\n",
      "Run: 46, exploration: 0.01, score: 117\n",
      "Run: 47, exploration: 0.01, score: 123\n",
      "Run: 48, exploration: 0.01, score: 101\n",
      "Run: 49, exploration: 0.01, score: 222\n",
      "Run: 50, exploration: 0.01, score: 142\n",
      "Run: 51, exploration: 0.01, score: 119\n",
      "Run: 52, exploration: 0.01, score: 230\n",
      "Run: 53, exploration: 0.01, score: 165\n",
      "Run: 54, exploration: 0.01, score: 57\n",
      "Run: 55, exploration: 0.01, score: 86\n",
      "Run: 56, exploration: 0.01, score: 62\n",
      "Run: 57, exploration: 0.01, score: 134\n",
      "Run: 58, exploration: 0.01, score: 151\n",
      "Run: 59, exploration: 0.01, score: 107\n",
      "Run: 60, exploration: 0.01, score: 176\n",
      "Run: 61, exploration: 0.01, score: 121\n",
      "Run: 62, exploration: 0.01, score: 148\n",
      "Run: 63, exploration: 0.01, score: 51\n",
      "Run: 64, exploration: 0.01, score: 125\n",
      "Run: 65, exploration: 0.01, score: 134\n",
      "Run: 66, exploration: 0.01, score: 131\n",
      "Run: 67, exploration: 0.01, score: 149\n",
      "Run: 68, exploration: 0.01, score: 83\n",
      "Run: 69, exploration: 0.01, score: 147\n",
      "Run: 70, exploration: 0.01, score: 141\n",
      "Run: 71, exploration: 0.01, score: 136\n",
      "Run: 72, exploration: 0.01, score: 146\n",
      "Run: 73, exploration: 0.01, score: 134\n",
      "Run: 74, exploration: 0.01, score: 140\n",
      "Run: 75, exploration: 0.01, score: 126\n",
      "Run: 76, exploration: 0.01, score: 126\n",
      "Run: 77, exploration: 0.01, score: 141\n",
      "Run: 78, exploration: 0.01, score: 159\n",
      "Run: 79, exploration: 0.01, score: 200\n",
      "Run: 80, exploration: 0.01, score: 174\n",
      "Run: 81, exploration: 0.01, score: 140\n",
      "Run: 82, exploration: 0.01, score: 243\n",
      "Run: 83, exploration: 0.01, score: 57\n",
      "Run: 84, exploration: 0.01, score: 385\n",
      "Run: 85, exploration: 0.01, score: 175\n",
      "Run: 86, exploration: 0.01, score: 307\n",
      "Run: 87, exploration: 0.01, score: 121\n",
      "Run: 88, exploration: 0.01, score: 500\n",
      "Run: 89, exploration: 0.01, score: 146\n",
      "Run: 90, exploration: 0.01, score: 221\n",
      "Run: 91, exploration: 0.01, score: 178\n",
      "Run: 92, exploration: 0.01, score: 225\n",
      "Run: 93, exploration: 0.01, score: 84\n",
      "Run: 94, exploration: 0.01, score: 60\n",
      "Run: 95, exploration: 0.01, score: 72\n",
      "Run: 96, exploration: 0.01, score: 500\n",
      "Run: 97, exploration: 0.01, score: 500\n",
      "Run: 98, exploration: 0.01, score: 500\n",
      "Run: 99, exploration: 0.01, score: 500\n",
      "Run: 100, exploration: 0.01, score: 500\n",
      "===============\n",
      "Episodes: 100, mean 100 episodes reward: 153.72\n",
      "===============\n",
      "Run: 101, exploration: 0.01, score: 290\n",
      "Run: 102, exploration: 0.01, score: 500\n",
      "Run: 103, exploration: 0.01, score: 500\n",
      "Run: 104, exploration: 0.01, score: 500\n",
      "Run: 105, exploration: 0.01, score: 500\n",
      "Run: 106, exploration: 0.01, score: 500\n",
      "Run: 107, exploration: 0.01, score: 500\n",
      "Run: 108, exploration: 0.01, score: 500\n",
      "Run: 109, exploration: 0.01, score: 500\n",
      "Run: 110, exploration: 0.01, score: 500\n",
      "Run: 111, exploration: 0.01, score: 493\n",
      "Run: 112, exploration: 0.01, score: 500\n",
      "Run: 113, exploration: 0.01, score: 500\n",
      "Run: 114, exploration: 0.01, score: 500\n",
      "Run: 115, exploration: 0.01, score: 371\n",
      "Run: 116, exploration: 0.01, score: 500\n",
      "Run: 117, exploration: 0.01, score: 500\n",
      "Run: 118, exploration: 0.01, score: 500\n",
      "Run: 119, exploration: 0.01, score: 500\n",
      "Run: 120, exploration: 0.01, score: 500\n",
      "Run: 121, exploration: 0.01, score: 500\n",
      "Run: 122, exploration: 0.01, score: 500\n",
      "Run: 123, exploration: 0.01, score: 500\n",
      "Run: 124, exploration: 0.01, score: 328\n",
      "Run: 125, exploration: 0.01, score: 12\n",
      "Run: 126, exploration: 0.01, score: 10\n",
      "Run: 127, exploration: 0.01, score: 500\n",
      "Run: 128, exploration: 0.01, score: 8\n",
      "Run: 129, exploration: 0.01, score: 10\n",
      "Run: 130, exploration: 0.01, score: 10\n",
      "Run: 131, exploration: 0.01, score: 10\n",
      "Run: 132, exploration: 0.01, score: 10\n",
      "Run: 133, exploration: 0.01, score: 8\n",
      "Run: 134, exploration: 0.01, score: 10\n",
      "Run: 135, exploration: 0.01, score: 10\n",
      "Run: 136, exploration: 0.01, score: 9\n",
      "Run: 137, exploration: 0.01, score: 10\n",
      "Run: 138, exploration: 0.01, score: 9\n",
      "Run: 139, exploration: 0.01, score: 9\n",
      "Run: 140, exploration: 0.01, score: 11\n",
      "Run: 141, exploration: 0.01, score: 10\n",
      "Run: 142, exploration: 0.01, score: 11\n",
      "Run: 143, exploration: 0.01, score: 10\n",
      "Run: 144, exploration: 0.01, score: 9\n",
      "Run: 145, exploration: 0.01, score: 12\n",
      "Run: 146, exploration: 0.01, score: 9\n",
      "Run: 147, exploration: 0.01, score: 10\n",
      "Run: 148, exploration: 0.01, score: 183\n",
      "Run: 149, exploration: 0.01, score: 500\n",
      "Run: 150, exploration: 0.01, score: 86\n",
      "Run: 151, exploration: 0.01, score: 15\n",
      "Run: 152, exploration: 0.01, score: 71\n",
      "Run: 153, exploration: 0.01, score: 77\n",
      "Run: 154, exploration: 0.01, score: 39\n",
      "Run: 155, exploration: 0.01, score: 14\n",
      "Run: 156, exploration: 0.01, score: 10\n",
      "Run: 157, exploration: 0.01, score: 15\n",
      "Run: 158, exploration: 0.01, score: 12\n",
      "Run: 159, exploration: 0.01, score: 10\n",
      "Run: 160, exploration: 0.01, score: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 161, exploration: 0.01, score: 11\n",
      "Run: 162, exploration: 0.01, score: 11\n",
      "Run: 163, exploration: 0.01, score: 10\n",
      "Run: 164, exploration: 0.01, score: 12\n",
      "Run: 165, exploration: 0.01, score: 9\n",
      "Run: 166, exploration: 0.01, score: 10\n",
      "Run: 167, exploration: 0.01, score: 8\n",
      "Run: 168, exploration: 0.01, score: 10\n",
      "Run: 169, exploration: 0.01, score: 10\n",
      "Run: 170, exploration: 0.01, score: 10\n",
      "Run: 171, exploration: 0.01, score: 10\n",
      "Run: 172, exploration: 0.01, score: 9\n",
      "Run: 173, exploration: 0.01, score: 10\n",
      "Run: 174, exploration: 0.01, score: 9\n",
      "Run: 175, exploration: 0.01, score: 11\n",
      "Run: 176, exploration: 0.01, score: 8\n",
      "Run: 177, exploration: 0.01, score: 8\n",
      "Run: 178, exploration: 0.01, score: 10\n",
      "Run: 179, exploration: 0.01, score: 8\n",
      "Run: 180, exploration: 0.01, score: 10\n",
      "Run: 181, exploration: 0.01, score: 10\n",
      "Run: 182, exploration: 0.01, score: 9\n",
      "Run: 183, exploration: 0.01, score: 9\n",
      "Run: 184, exploration: 0.01, score: 9\n",
      "Run: 185, exploration: 0.01, score: 9\n",
      "Run: 186, exploration: 0.01, score: 9\n",
      "Run: 187, exploration: 0.01, score: 8\n",
      "Run: 188, exploration: 0.01, score: 9\n",
      "Run: 189, exploration: 0.01, score: 10\n",
      "Run: 190, exploration: 0.01, score: 9\n",
      "Run: 191, exploration: 0.01, score: 10\n",
      "Run: 192, exploration: 0.01, score: 8\n",
      "Run: 193, exploration: 0.01, score: 9\n",
      "Run: 194, exploration: 0.01, score: 10\n",
      "Run: 195, exploration: 0.01, score: 10\n",
      "Run: 196, exploration: 0.01, score: 10\n",
      "Run: 197, exploration: 0.01, score: 10\n",
      "Run: 198, exploration: 0.01, score: 13\n"
     ]
    }
   ],
   "source": [
    "# dqn = train('SunblazeCartPole-v0')\n",
    "env = \"CartPole-v1\"\n",
    "dqn = train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(dqn, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
